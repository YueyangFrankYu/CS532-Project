# -*- coding: utf-8 -*-
"""PreProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EbAnLMRnH1hCw3_aC3P0nYZ5C-dpXo7i
"""

# Install necessary library
!pip install pyspark

# Importing required libraries
import os
import nltk
import pyspark.sql.functions as F
import zipfile
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType, FloatType
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from google.colab import drive

# Initialize Spark session
spark = SparkSession.builder.appName("DataPreprocessing").getOrCreate()

# Mount Google Drive to access files
drive.mount('/content/drive')

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Initialize NLTK lemmatizer
lemmatizer = WordNetLemmatizer()

# Define file paths
zip_file_path = '/content/drive/MyDrive/Dataset.zip'
extracted_folder_path = '/content/extractedFiles'

# Extract files from the zip archive
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder_path)

# Display success message after extracting files
print("File has been successfully unzipped to:", extracted_folder_path)

# Set the path to the dataset
dataset_path = '/content/extractedFiles'

# Read the dataset into a Spark DataFrame
df = spark.read.option("header", "true").csv(dataset_path)

# Display the number of rows in the DataFrame
df.count()

# Display the "content" column of the DataFrame
(df.select("content")).show()

# Remove duplicate rows, fill missing values, and convert "content" to lowercase
df = df.dropDuplicates()
df = df.fillna('', subset=['content'])
df = df.withColumn("rawData", df['content'])  # Preserve original content
df = df.withColumn("content", F.lower(df["content"]))

# Define a function to clean text
def clean_text(text):
    text = re.sub(r'http\S+|www\S+|@\w+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = ' '.join(text.split())
    return text

# Apply the clean_text function to the "content" column
clean_text_udf = udf(clean_text, StringType())
df = df.withColumn("content", clean_text_udf(df["content"]))

# Tokenize the "content" column
tokenizer = Tokenizer(inputCol="content", outputCol="tokens_unlemmatized")
df = tokenizer.transform(df)

# Define stop words and remove them from the tokenized content
stop_words = list(set(word_tokenize(' '.join(stopwords.words('english')))))
stop_words.extend(['russian', 'u'])
remover = StopWordsRemover(inputCol="tokens_unlemmatized", outputCol="tokens", stopWords=stop_words)
df = remover.transform(df)

# Lemmatize the tokenized content
lemmatize_udf = udf(lambda tokens: ' '.join([lemmatizer.lemmatize(word) for word in tokens]), (StringType()))
df = df.withColumn("tokens", lemmatize_udf(df["tokens"]))

# Display the "content" and "tokens" columns of the DataFrame
df.select("content", "tokens").show(truncate=False)

# Display the "rawData", "content", and "tokens" columns of the DataFrame
df.select("rawData", "content", "tokens").show()

# Download VADER lexicon for sentiment analysis
nltk.download('vader_lexicon')

# Initialize VADER Sentiment Intensity Analyzer
analyzer = SentimentIntensityAnalyzer()

# Define a function to analyze sentiment using VADER
def analyze_sentiment(text):
    compound = analyzer.polarity_scores(text)["compound"]
    return float(compound)

# Apply the analyze_sentiment function to the "content" column
analyze_sentiment_udf = udf(analyze_sentiment, FloatType())
df = df.withColumn("sentiment", analyze_sentiment_udf(df["content"]))

# Display the "content", "tokens", and "sentiment" columns of the DataFrame
df.select("content", "tokens", "sentiment").show(truncate=False)

# Categorize sentiment into Negative, Neutral, or Positive
from pyspark.sql.functions import when

conditions = [
    (df['sentiment'] <= -0.5),
    (df['sentiment'] > -0.5) & (df['sentiment'] < 0.5),
    (df['sentiment'] >= 0.5)
]

values = ['Negative', 'Neutral', 'Positive']

df = df.withColumn('Category', when(conditions[0], values[0])
                                .when(conditions[1], values[1])
                                .when(conditions[2], values[2])
                                .otherwise(None))

# Display the "content", "tokens", "sentiment", and "Category" columns of the DataFrame
df.select("content", "tokens", "sentiment", "Category").show(truncate=False)

# Select specific columns for output
selected_columns = df.select("rawData", "tokens", "sentiment", "Category")

# Define the output path for saving the processed data
output_path = "/content/drive/MyDrive/train_28Nov"

# Write the selected columns to a CSV file
selected_columns.write.csv(output_path, header=True, mode="overwrite", sep='\t')
