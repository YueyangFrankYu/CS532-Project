{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "\n",
    "path = '../data/data_raw'\n",
    "paths = [path+'/'+str(i)+'.csv' for i in range(23)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleanup\n",
    "df = spark.read.csv(paths,header=True)\n",
    "df = df.dropna()\n",
    "# train test split\n",
    "(trainData, testData) = df.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model choice\n",
    "According to literature review, Logistic regression is just as effective as naive bayes or support vector machines for sentiment analysis \\*1\n",
    "- TF-IDF process:\n",
    "    - CounvtVectorizer converts words into sparse matrix of vectors, vocabsize restrict the top x words to be used, minDF (minimum document frequency) ignores words that appear less than x amount of x percent in the entire document(s). \n",
    "    - Inverse document frequnecy (IDF) adds a discount to words that appear frequently in the text\n",
    "- execution speed dependency:\n",
    "    - vocabsize/minDF: controls the sparsity of model\n",
    "    - parallelization for matrix multiplications/ softmax multi-logistic regression and its gradient calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline \n",
    "tokenizer = Tokenizer(inputCol=\"tokens\", outputCol=\"words\")\n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"cv\", vocabSize=30000, minDF=5)\n",
    "idf = IDF(inputCol='cv',outputCol='features',minDocFreq=5)\n",
    "label = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[label, tokenizer, countVectors, idf, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Due to large amount of tweets (~2mil), 3 fold CV is used to determine the best elastic net and regularization parameter - ***Warning long runtime*** \\\n",
    "Accuracy, precision and recall generally decrease as more regularization strength is applied. \\\n",
    "~ 85% - 75% accuracy for regparam 0.0 - 0.1, elastic net 0, 0.5 ,1 \\\n",
    "*Important caveat*: The data was not manually labeled, instead it's done with NLTK VADER, so it is comparing how similar our model output is to the NLTK version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search w/ 3 fold cv \n",
    "# %time\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[label, tokenizer, countVectors, idf, lr])\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1]).addGrid(lr.elasticNetParam, [0., 0.5, 1.0]).build()\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(trainData)\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"CV Accuracy:\", accuracy)\n",
    "print(\"CV Precision:\", precision)\n",
    "print(\"CV Recall:\", recall)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "cvModel.getEstimatorParamMaps()[np.argmax(cvModel.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7782525881551807\n",
      "Precision: 0.7763460163068651\n",
      "Recall: 0.7782525881551807\n"
     ]
    }
   ],
   "source": [
    "# train with selected params\n",
    "lr = LogisticRegression(regParam = 0.01, maxIter= 100, elasticNetParam = 0.)\n",
    "pipeline = Pipeline(stages=[label, tokenizer, countVectors, idf, lr])\n",
    "\n",
    "lrModel = pipeline.fit(trainData)\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# lrModel.write().overwrite().save(\"/Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation\n",
    "1. Samar Al-Saqqa, Ghazi Al-Naymat, Arafat Awajan,\n",
    "A Large-Scale Sentiment Data Classification for Online Reviews Under Apache Spark,\n",
    "Procedia Computer Science,Volume 141,2018,Pages 183-189,ISSN 1877-0509,https://doi.org/10.1016/j.procs.2018.10.166."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
