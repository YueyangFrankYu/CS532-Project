{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "\n",
    "path = './data_raw'\n",
    "paths = [path+'/'+str(i)+'.csv' for i in range(23)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "df = spark.read.option(\"delimiter\", \"\\t\").csv(paths,header=True)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rawData: string (nullable = true)\n",
      " |-- tokens: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(trainData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\n",
    "testData.write.csv('./data_test', header=True, mode=\"overwrite\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1349850"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404376"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rawData: string (nullable = true)\n",
      " |-- tokens: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"tokens\", outputCol=\"words\")\n",
    "trainData = tokenizer.transform(trainData)\n",
    "trainData = trainData.drop(\"rawData\").drop(\"tokens\")\n",
    "trainData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData2 = tokenizer.transform(testData)\n",
    "testData2 = testData2.drop(\"rawData\").drop(\"tokens\")\n",
    "testData2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline \n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"cv\", vocabSize=30000, minDF=5)\n",
    "idf = IDF(inputCol='cv',outputCol='features',minDocFreq=5)\n",
    "label = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[label, countVectors, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 23:56:21 WARN DAGScheduler: Broadcasting large task binary with size 1571.8 KiB\n",
      "23/11/27 23:56:34 WARN DAGScheduler: Broadcasting large task binary with size 1571.8 KiB\n",
      "23/11/27 23:56:46 WARN DAGScheduler: Broadcasting large task binary with size 1571.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8697449898114626\n",
      "Precision: 0.8675089703191299\n",
      "Recall: 0.8697449898114626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lrModel = pipeline.fit(trainData)\n",
    "predictions = lrModel.transform(testData2)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "lrModel.write().overwrite().save(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "load_model = PipelineModel.load(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/28 00:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1570.7 KiB\n",
      "23/11/28 00:00:20 WARN DAGScheduler: Broadcasting large task binary with size 1570.7 KiB\n",
      "23/11/28 00:00:31 WARN DAGScheduler: Broadcasting large task binary with size 1570.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = load_model.transform(testData2)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- cv: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/28 00:01:22 WARN DAGScheduler: Broadcasting large task binary with size 1558.1 KiB\n",
      "[Stage 325:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+--------------------+\n",
      "|prediction|label|Category|            features|\n",
      "+----------+-----+--------+--------------------+\n",
      "|       0.0|  0.0| Neutral|(30000,[2,116,123...|\n",
      "|       0.0|  0.0| Neutral|(30000,[61,125],[...|\n",
      "|       0.0|  0.0| Neutral|(30000,[0,5,17,18...|\n",
      "|       0.0|  0.0| Neutral|(30000,[3325],[7....|\n",
      "|       0.0|  0.0| Neutral|(30000,[2065,1851...|\n",
      "|       0.0|  0.0| Neutral|(30000,[63],[3.65...|\n",
      "|       0.0|  0.0| Neutral|(30000,[0,8,14,16...|\n",
      "|       0.0|  0.0| Neutral|(30000,[15,63],[2...|\n",
      "|       0.0|  0.0| Neutral|(30000,[277,698,9...|\n",
      "|       0.0|  0.0| Neutral|(30000,[3,63,487,...|\n",
      "|       0.0|  0.0| Neutral|(30000,[15,44],[2...|\n",
      "|       0.0|  0.0| Neutral|(30000,[2647],[7....|\n",
      "|       0.0|  0.0| Neutral|(30000,[16,328],[...|\n",
      "|       0.0|  0.0| Neutral|(30000,[62,256,38...|\n",
      "|       0.0|  0.0| Neutral|(30000,[62,256,38...|\n",
      "|       0.0|  0.0| Neutral|(30000,[0,7,35,10...|\n",
      "|       0.0|  0.0| Neutral|(30000,[1,5,14,20...|\n",
      "|       0.0|  0.0| Neutral|(30000,[63,1631,2...|\n",
      "|       0.0|  0.0| Neutral|(30000,[16,2093],...|\n",
      "|       0.0|  0.0| Neutral|(30000,[534,975,1...|\n",
      "+----------+-----+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"label\", \"Category\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
